{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249d8706",
   "metadata": {},
   "source": [
    "# Walk-through - Virtualizing NetCDFs from Amazon's Open Data Program\n",
    "\n",
    "Let's start with the first example from the [VirtualiZarr homepage](https://virtualizarr.readthedocs.io/en/stable/index.html#features).\n",
    "\n",
    "This example uses the [NASA Earth Exchange Global Daily Downscaled Projections](https://registry.opendata.aws/nex-gddp-cmip6/) (NEX-GDDP-CMIP6) from the Registry of Open Data on AWS. The virtualization\n",
    "process will be much faster if run proximal to the data in AWS's us-west-2 region.\n",
    "\n",
    "Creating the virtual dataset looks quite similar to how we normally open data with [xarray](https://docs.xarray.dev/en/stable/index.html#module-xarray), but there are a few notable differences that are shown through this example.\n",
    "\n",
    "First, import the necessary functions and classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dfe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import icechunk\n",
    "import obstore\n",
    "from virtualizarr import open_virtual_dataset, open_virtual_mfdataset\n",
    "from virtualizarr.parsers import HDFParser\n",
    "from virtualizarr.registry import ObjectStoreRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fe3f7",
   "metadata": {},
   "source": [
    "Zarr can emit a lot of warnings about Numcodecs not being including in the Zarr version 3 specification yet -- let's suppress those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f334fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c488b1",
   "metadata": {},
   "source": [
    "We can use Obstore's [`obstore.store.from_url`](https://developmentseed.org/obstore/latest/api/store/#obstore.store.from_url) convenience method to create an [`ObjectStore`](https://developmentseed.org/obstore/latest/api/store/#obstore.store.ObjectStore) that can fetch data from the specified URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"s3://nex-gddp-cmip6\"\n",
    "path = \"NEX-GDDP-CMIP6/ACCESS-CM2/ssp126/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_ssp126_r1i1p1f1_gn_2015_v2.0.nc\"\n",
    "store = obstore.store.from_url(bucket, region=\"us-west-2\", skip_signature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdcd3d",
   "metadata": {},
   "source": [
    "We also need to create an [ObjectStoreRegistry](https://virtualizarr.readthedocs.io/en/stable/api/developer.html#virtualizarr.registry.ObjectStoreRegistry) that maps the URL structure to the ObjectStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ObjectStoreRegistry({bucket: store})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1f3e6",
   "metadata": {},
   "source": [
    "Now, let's create a parser instance and create a virtual dataset by passing the URL, parser, and registry to [`virtualizarr.open_virtual_dataset`](https://virtualizarr.readthedocs.io/en/stable/api/virtualizarr.html#virtualizarr.open_virtual_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4325c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HDFParser()\n",
    "vds = open_virtual_dataset(\n",
    "    url=f\"{bucket}/{path}\",\n",
    "    parser=parser,\n",
    "    registry=registry,\n",
    "    loadable_variables=[],\n",
    ")\n",
    "print(vds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78b217",
   "metadata": {},
   "source": [
    "Since we specified `loadable_variables=[]`, no data has been loaded or copied in this process. We have merely created an in-memory lookup table that points to the location of chunks in the original netCDF when data is needed later on. The default behavior (`loadable_variables=None`) will load data associated with coordinates but not data variables. The size represents the size of the original dataset - you can see the size of the virtual dataset using the vz accessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original dataset size: {vds.nbytes} bytes\")\n",
    "print(f\"Virtual dataset size: {vds.vz.nbytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c6613",
   "metadata": {},
   "source": [
    "VirtualiZarr's other top-level function is [`virtualizarr.open_virtual_mfdataset`](https://virtualizarr.readthedocs.io/en/stable/api/virtualizarr.html#virtualizarr.open_virtual_mfdataset), which can open and virtualize multiple data sources into a single virtual dataset, similar to how [`xarray.open_mfdataset`](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) opens multiple data files as a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    f\"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/ssp126/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_ssp126_r1i1p1f1_gn_{year}_v2.0.nc\"\n",
    "    for year in range(2015, 2017)\n",
    "]\n",
    "vds = open_virtual_mfdataset(urls, parser=parser, registry=registry)\n",
    "print(vds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f9ac8",
   "metadata": {},
   "source": [
    "The magic of VirtualiZarr is that you can persist the virtual dataset to disk in a chunk references format such as Icechunk, meaning that the work of constructing the single coherent dataset only needs to happen once. For subsequent data access, you can use xarray.open_zarr to open that Icechunk store, which on object storage is far faster than using xarray.open_mfdataset to open the the original non-cloud-optimized files.\n",
    "\n",
    "Let's persist the Virtual dataset using Icechunk. Here we store the dataset in a memory store but in most cases you'll store the virtual dataset in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "icechunk_store = icechunk.in_memory_storage()\n",
    "repo = icechunk.Repository.create(icechunk_store)\n",
    "session = repo.writable_session(\"main\")\n",
    "vds.vz.to_icechunk(session.store)\n",
    "session.commit(\"Create virtual store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03a005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
