[
  {
    "objectID": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html",
    "href": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html",
    "title": "Creating a STAC Collection for a Virtual Icechunk Store",
    "section": "",
    "text": "There is a virtual icechunk store that is publicly available at: s3://nasa-waterinsight/virtual-zarr-store/NLDAS-3-icechunk/\nThis notebook goes through the current thinking for how you would set up a STAC collection that points to that virtual icechunk store and provides all the information a user needs to interact with the virtual zarr store programatically or via a web UI.\nimport json\nimport datetime\n\nimport icechunk\nimport pystac\nimport xstac\nimport zarr\n\nimport xarray as xr\nZarr can emit a lot of warnings about Numcodecs not being including in the Zarr version 3 specification yet – let’s suppress those.\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n    category=UserWarning,\n)\nThese are the PRs that need to land before you can open the virtual icechunk store with zarr directly:\nUntil then:\nstorage = icechunk.s3_storage(\n    bucket=\"nasa-waterinsight\",\n    prefix=\"virtual-zarr-store/NLDAS-3-icechunk/\",\n    region=\"us-west-2\",\n    anonymous=True,\n)\nThe bucket and prefix are from the icechunk href. The anonymous=True needs to come from somewhere else.\nconfig = icechunk.RepositoryConfig.default()\nconfig.set_virtual_chunk_container(\n    icechunk.VirtualChunkContainer(\n        \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\",\n        icechunk.s3_store(region=\"us-west-2\")\n    )\n)\nvirtual_credentials = icechunk.containers_credentials(\n    {\n        \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\": icechunk.s3_anonymous_credentials()\n    }\n)\nHere we need the href for the internal storage bucket(s) (composed of bucket and prefix) and we need the region of that bucket. Then we need some way of providing credentials.\nrepo = icechunk.Repository.open(\n    storage=storage,\n    config=config,\n    authorize_virtual_chunk_access=virtual_credentials,\n)\n\nsession = repo.readonly_session(snapshot_id='YTNGFY4WY9189GEH1FNG')\nds = xr.open_zarr(session.store, consolidated=False, zarr_format=3)\nLast of all we need a way of specifying that we are looking at icechunk here as well as the standard fields: consolodated, zarr_format that are already included in the STAC Zarr extension.\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 51TB\nDimensions:   (time: 8399, lat: 6500, lon: 11700)\nCoordinates:\n  * lon       (lon) float64 94kB -169.0 -169.0 -169.0 ... -52.03 -52.01 -52.0\n  * lat       (lat) float64 52kB 7.005 7.015 7.025 7.035 ... 71.97 71.98 71.99\n  * time      (time) datetime64[ns] 67kB 2001-01-02 2001-01-03 ... 2024-01-01\nData variables:\n    LWdown    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Rainf     (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    PSurf     (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Tair_max  (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Tair      (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Wind_E    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Qair      (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Wind_N    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    SWdown    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Tair_min  (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\nAttributes: (12/17)\n    missing_value:          -9999.0\n    time_definition:        daily\n    shortname:              NLDAS_FOR0010_D_3.0\n    title:                  NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3...\n    version:                3.0 beta\n    institution:            NASA GSFC\n    ...                     ...\n    websites:               https://ldas.gsfc.nasa.gov/nldas/v3/ ; https://li...\n    MAP_PROJECTION:         EQUIDISTANT CYLINDRICAL\n    SOUTH_WEST_CORNER_LAT:  7.005000114440918\n    SOUTH_WEST_CORNER_LON:  -168.9949951171875\n    DX:                     0.009999999776482582\n    DY:                     0.009999999776482582xarray.DatasetDimensions:time: 8399lat: 6500lon: 11700Coordinates: (3)lon(lon)float64-169.0 -169.0 ... -52.01 -52.0units :degrees_eaststandard_name :longitudelong_name :longitudevmin :-168.9949951171875vmax :-52.00499725341797array([-168.994995, -168.985001, -168.974991, ...,  -52.025002,  -52.014999,\n        -52.004997], shape=(11700,))lat(lat)float647.005 7.015 7.025 ... 71.98 71.99units :degrees_northstandard_name :latitudelong_name :latitudevmin :7.005000114440918vmax :71.9949951171875array([ 7.005   ,  7.015   ,  7.025   , ..., 71.974998, 71.984993, 71.994995],\n      shape=(6500,))time(time)datetime64[ns]2001-01-02 ... 2024-01-01long_name :timetime_increment :one daybegin_date :20010101begin_time :000000end_date :20010101end_time :235959array(['2001-01-02T00:00:00.000000000', '2001-01-03T00:00:00.000000000',\n       '2001-01-04T00:00:00.000000000', ..., '2023-12-30T00:00:00.000000000',\n       '2023-12-31T00:00:00.000000000', '2024-01-01T00:00:00.000000000'],\n      shape=(8399,), dtype='datetime64[ns]')Data variables: (10)LWdown(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :W m-2standard_name :Surface incident longwave radiationlong_name :Longwave radiation flux downwards (surface)cell_methods :time: meanvmin :126.3478012084961vmax :450.3310546875\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nRainf(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :kg m-2standard_name :Total precipitationlong_name :Total precipitationcell_methods :time: sumvmin :0.0vmax :60.617774963378906\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nPSurf(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Pastandard_name :Surface pressurelong_name :Surface pressurecell_methods :time: meanvmin :46024.41796875vmax :103161.1796875\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nTair_max(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Daily maximum near surface air temperaturelong_name :Daily maximum 2-meter above ground Temperaturecell_methods :time: maximumvmin :230.1659393310547vmax :312.7441711425781\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nTair(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Near surface air temperaturelong_name :2-meter above ground Temperaturecell_methods :time: meanvmin :228.31163024902344vmax :304.30316162109375\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nWind_E(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :m s-1standard_name :Near surface eastward wind componentlong_name :10-meter above ground Zonal wind speedcell_methods :time: meanvmin :-12.775843620300293vmax :11.65600872039795\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nQair(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :kg kg-1standard_name :Near surface specific humiditylong_name :2-meter above ground Specific humiditycell_methods :time: meanvmin :5.8038440329255536e-05vmax :0.021006619557738304\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nWind_N(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :m s-1standard_name :Near surface northward wind componentlong_name :10-meter above ground Meridional wind speedcell_methods :time: meanvmin :-10.617785453796387vmax :11.41434097290039\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nSWdown(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :W m-2standard_name :Surface incident shortwave radiationlong_name :Shortwave radiation flux downwards (surface)cell_methods :time: meanvmin :0.0vmax :288.2958068847656\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nTair_min(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Daily minimum near surface air temperaturelong_name :Daily minimum 2-meter above ground Temperaturecell_methods :time: minimumvmin :227.1617889404297vmax :300.4527282714844\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Index([ -168.9949951171875, -168.98500061035156, -168.97499084472656,\n       -168.96499633789062,  -168.9550018310547,  -168.9449920654297,\n       -168.93499755859375, -168.92498779296875,  -168.9149932861328,\n       -168.90499877929688,\n       ...\n       -52.095001220703125, -52.084999084472656,  -52.07499694824219,\n        -52.06499481201172,  -52.05500030517578,  -52.04499816894531,\n       -52.034996032714844, -52.025001525878906,  -52.01499938964844,\n        -52.00499725341797],\n      dtype='float64', name='lon', length=11700))latPandasIndexPandasIndex(Index([7.005000114440918, 7.015000343322754, 7.025000095367432,\n       7.035000324249268, 7.045000076293945, 7.055000305175781,\n       7.065000057220459, 7.075000286102295, 7.085000038146973,\n       7.095000267028809,\n       ...\n       71.90499877929688, 71.91499328613281, 71.92499542236328,\n       71.93499755859375, 71.94499206542969, 71.95499420166016,\n       71.96499633789062,  71.9749984741211, 71.98499298095703,\n        71.9949951171875],\n      dtype='float64', name='lat', length=6500))timePandasIndexPandasIndex(DatetimeIndex(['2001-01-02', '2001-01-03', '2001-01-04', '2001-01-05',\n               '2001-01-06', '2001-01-07', '2001-01-08', '2001-01-09',\n               '2001-01-10', '2001-01-11',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', name='time', length=8399, freq=None))Attributes: (17)missing_value :-9999.0time_definition :dailyshortname :NLDAS_FOR0010_D_3.0title :NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3.0 - *BETA*version :3.0 betainstitution :NASA GSFCdoi :TBDhistory :created on date: 2025-06-07T15:04:40reference :Maina-et-al_to-be-submitted; doi:TBDconventions :CF-1.6standard :ALMA v3 ; https://www.lmd.jussieu.fr/~polcher/ALMA/websites :https://ldas.gsfc.nasa.gov/nldas/v3/ ; https://lis.gsfc.nasa.gov/MAP_PROJECTION :EQUIDISTANT CYLINDRICALSOUTH_WEST_CORNER_LAT :7.005000114440918SOUTH_WEST_CORNER_LON :-168.9949951171875DX :0.009999999776482582DY :0.009999999776482582\nOk! We have the xarray dataset lazily opened by accessing only the virtual icechunk store! Now the goal is to create a STAC collection that describes that virtual icechunk store and contains all the information we need for accessing it (all the inputs to the functions above).",
    "crumbs": [
      "Examples",
      "STAC collection for virtual store"
    ]
  },
  {
    "objectID": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html#extract-metadata-for-stac-collection",
    "href": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html#extract-metadata-for-stac-collection",
    "title": "Creating a STAC Collection for a Virtual Icechunk Store",
    "section": "Extract metadata for STAC collection",
    "text": "Extract metadata for STAC collection\nNow let’s see how xstac can help us extract the variables and represent them using the Datacube STAC Extension.\n\nThis section takes inspiration from https://github.com/stac-utils/xstac/blob/main/examples/nasa-nex-gddp-cmip6/generate.py\n\nWe’ll start with some of the hard-coded values that will need to be provided for any given dataset. As much as possible these should be lifted directly from the xr.Dataset attrs.\n\ncollection_id = \"nldas-3\"\ndescription = (\n    \"NLDAS-3 provides a fine-scale (1 km) meteorological forcing (precipitation) in \"\n    \"both retrospective and near real-time over North and Central America, including \"\n    \"Alaska, Hawaii, and Puerto Rico, by leveraging high-quality gauge, satellite, \"\n    \"and model datasets through advanced data assimilation methods. Read more: \"\n    \"https://ldas.gsfc.nasa.gov/nldas/v3\"\n)\nproviders = [\n    pystac.Provider(\n        name=\"NLDAS\",\n        roles=[\"producer\", \"processor\", \"licensor\"],\n        url=\"https://ldas.gsfc.nasa.gov/nldas\"\n    )\n]\n\nI want to draw special attention to how we can use the Storage STAC Extension to capture that this particular bucket can be accessed anonymously. We can also capture the region within this blob.\n\nstorage_schemes = {\n    \"aws-s3-nasa-waterinsight\": {\n        \"type\": \"aws-s3\",\n        \"platform\": \"https://{bucket}.s3.{region}.amazonaws.com\",\n        \"bucket\": \"nasa-waterinsight\",\n        \"region\": \"us-west-2\",\n        \"anonymous\": True,\n    }\n}\n\nNow let’s configure some metadata that can be gotten from the xr.Dataset itself.\n\ntitle = ds.attrs[\"title\"]\nextents = pystac.Extent(\n    spatial=pystac.SpatialExtent(bboxes=[list(ds.rio.bounds())]),\n    temporal=pystac.TemporalExtent(\n        intervals=[\n            datetime.datetime.fromisoformat(str(ds.time.min().values)),\n            datetime.datetime.fromisoformat(str(ds.time.max().values))\n        ]\n    ),\n)\n\nNow that we have all those values set, create a pystac.Collection:\n\ntemplate = pystac.Collection(\n    collection_id,\n    description=description,\n    extent=extents,\n    extra_fields={\"storage:schemes\": storage_schemes, \"item_assets\": {}},\n    providers=providers,\n    title=title,\n    stac_extensions = [\n        \"https://stac-extensions.github.io/storage/v2.0.0/schema.json\",\n    ]\n)\n\nNow that we have a preliminary version of the STAC Collection we can pass it off to xstac to pull out the variables and dims using the Datacube STAC Extension.\n\ncollection = xstac.xarray_to_stac(\n    ds,\n    template,\n    temporal_dimension=\"time\",\n    x_dimension=\"lon\",\n    y_dimension=\"lat\",\n    reference_system=4326\n)\n\nWith that collection in hand we can create an asset that points to the virtual icechunk store and add it as a collection-level asset.\nThe main concern of the asset is how to access the data. So we need the asset to contain all the information we need to pass into icechunk functions when opening the virtual icechunk store. We also need to specify how to access the legacy files which potentially sit in their own bucket. We can use the Virtual Assets STAC Extension to capture that.\n\ncollection.add_asset(\n    \"nldas-3@YTNGFY4WY9189GEH1FNG\",\n    pystac.Asset(\n        \"s3://nasa-waterinsight/virtual-zarr-store/NLDAS-3-icechunk/\",\n        title=\"NLDAS-3 Virtual Zarr Store\",\n        media_type=\"application/vnd.zarr+icechunk\",  # I made this up - is there a better one to use?\n        roles=[\"data\", \"references\", \"virtual\", \"latest-version\"],\n        extra_fields={\n            \"zarr:consolidated\": False,\n            \"zarr:zarr_format\": 3,\n            \"icechunk:snapshot_id\": \"YTNGFY4WY9189GEH1FNG\",\n            \"storage:refs\": [\n                \"aws-s3-nasa-waterinsight\"\n            ],\n            \"vrt:hrefs\": [\n                {\n                    \"key\": \"nldas-3-legacy-bucket\",\n                    \"href\": \"https://raw.githubusercontent.com/NASA-IMPACT/dse-virtual-zarr-workshop/refs/heads/main/docs/examples/collection.json#/assets/nldas-3-legacy-bucket\"\n                },\n            ],\n        }\n    )\n)\n\n\ncollection.add_asset(\n    \"nldas-3-legacy-bucket\",\n    pystac.Asset(\n        \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\",\n        title=\"NLDAS-3 Legacy Bucket\",\n        media_type=\"application/x-netcdf\",\n        roles=[\"data\"],\n        extra_fields={\n            \"storage:refs\": [\n                \"aws-s3-nasa-waterinsight\"\n            ],\n        }\n    )\n)\n\nWe can also add information about how to render each variable. This uses the Render STAC Extension and specifies how applications (for instance titiler) should represent each variable visually.\n\nrenders = {}\nfor k in ds:\n    if k.startswith(\"LW\"):\n        colormap_name = \"inferno\"\n    elif k.startswith(\"PS\"):\n        colormap_name = \"viridis\"\n    elif k.startswith(\"Q\"):\n        colormap_name = \"plasma\"\n    elif k.startswith(\"Rain\"):\n        colormap_name = \"cfastie\"\n    elif k.startswith(\"SW\"):\n        colormap_name = \"magma\"\n    elif k.startswith(\"T\"):\n        colormap_name = \"RdYlBu_r\"\n    elif k.startswith(\"Wind\"):\n        colormap_name = \"PuOr\"\n        \n    renders[k] = {\n        \"title\": ds[k].attrs[\"long_name\"],\n        \"assets\": [\"nldas-3\"],\n        \"resampling\": \"average\",\n        \"colormap_name\": colormap_name,\n        \"rescale\": [[ds[k].attrs[\"vmin\"], ds[k].attrs[\"vmax\"]]],\n        \"backend\": \"xarray\",\n    }\n\ncollection.extra_fields[\"renders\"] = renders\n\nLast of all we will add the Zarr STAC Extension and the Render STAC Extension to the list of stac_extensions. We need to do this after adding the asset because they relies on there being assets in the collection.\n\ncollection.stac_extensions = list(set(\n    collection.stac_extensions + [\n        \"https://stac-extensions.github.io/render/v2.0.0/schema.json\",\n        \"https://stac-extensions.github.io/virtual-assets/v1.0.0/schema.json\",\n        \"https://stac-extensions.github.io/zarr/v1.1.0/schema.json\",\n        \"https://stac-extensions.github.io/version/v1.2.0/schema.json\",\n    ]\n))\n\nValidate the collection:\n\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.1.0/collection-spec/json-schema/collection.json',\n 'https://stac-extensions.github.io/virtual-assets/v1.0.0/schema.json',\n 'https://stac-extensions.github.io/datacube/v2.2.0/schema.json',\n 'https://stac-extensions.github.io/storage/v2.0.0/schema.json',\n 'https://stac-extensions.github.io/render/v2.0.0/schema.json',\n 'https://stac-extensions.github.io/zarr/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/version/v1.2.0/schema.json']\n\n\nDump the collection to json\n\nwith open(\"collection.json\", \"w\") as f:\n    json.dump(collection.to_dict(), f, indent=2)",
    "crumbs": [
      "Examples",
      "STAC collection for virtual store"
    ]
  },
  {
    "objectID": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html#read-the-virtual-icechunk-using-collection-level-asset",
    "href": "examples/03_STAC_Collection_for_Virtual_Icechunk_Store.html#read-the-virtual-icechunk-using-collection-level-asset",
    "title": "Creating a STAC Collection for a Virtual Icechunk Store",
    "section": "Read the virtual icechunk using collection-level asset",
    "text": "Read the virtual icechunk using collection-level asset\nLet’s create a function that opens the virtual icechunk store as a virtual asset using only the STAC metadata.\n\ndef read_virtual_asset(collection: pystac.Collection, asset_key: str):\n    \"\"\" Read a collection-level virtual icechunk asset \"\"\"\n    virtual_asset = collection.assets[asset_key]\n\n    # --- Create icechunk storage for virtual store\n    storage_refs = virtual_asset.extra_fields[\"storage:refs\"]\n    if len(storage_refs) != 1:\n        raise ValueError(\"Only supports one storage:ref per virtual asset\")\n    \n    storage_scheme = collection.extra_fields[\"storage:schemes\"].get(storage_refs[0])\n    if not storage_scheme[\"type\"] == \"aws-s3\":\n        raise ValueError(\"Only S3 buckets are currently supported\")\n    \n    bucket = storage_scheme[\"bucket\"]\n    region = storage_scheme[\"region\"]\n    anonymous = storage_scheme.get(\"anonymous\", False)\n    prefix = virtual_asset.href.split(f\"{bucket}/\")[1]\n    \n    storage = icechunk.s3_storage(\n        bucket=bucket,\n        prefix=prefix,\n        region=region,\n        anonymous=anonymous,\n    )\n\n    # --- Create icechunk config object for chunk store\n    legacy_buckets = virtual_asset.extra_fields[\"vrt:hrefs\"]\n\n    if len(legacy_buckets) != 1:\n        raise ValueError(\"Only supports one vrt:href per virtual asset\")\n    \n    legacy_asset = collection.assets[legacy_buckets[0][\"key\"]]\n    href = legacy_asset.href\n    \n    storage_refs = legacy_asset.extra_fields[\"storage:refs\"]\n    if len(storage_refs) != 1:\n        raise ValueError(\"Only supports one storage:ref per legacy asset\")\n    \n    storage_scheme = collection.extra_fields[\"storage:schemes\"].get(storage_refs[0])\n    if not storage_scheme[\"type\"] == \"aws-s3\":\n        raise ValueError(\"Only S3 buckets are currently supported\")\n    \n    bucket = storage_scheme[\"bucket\"]\n    region = storage_scheme[\"region\"]\n    anonymous = storage_scheme.get(\"anonymous\", False)\n    \n    config = icechunk.RepositoryConfig.default()\n    config.set_virtual_chunk_container(\n        icechunk.VirtualChunkContainer(\n            href,\n            icechunk.s3_store(region=region)\n        )\n    )\n    if anonymous:\n        virtual_credentials = icechunk.containers_credentials(\n            {\n                href: icechunk.s3_anonymous_credentials()\n            }\n        )\n    else:\n        raise ValueError(\"Only anonymous S3 buckets are currently supported\")\n\n    # --- Open icechunk session at a particular snapshot\n    repo = icechunk.Repository.open(\n        storage=storage,\n        config=config,\n        authorize_virtual_chunk_access=virtual_credentials,\n    )\n\n    # TODO: should look for exactly one of \"snapshot_id\", \"branch\", \"tag\"\n    snapshot_id = virtual_asset.extra_fields[\"icechunk:snapshot_id\"]\n    session = repo.readonly_session(snapshot_id=snapshot_id)\n\n    # --- Open store as an xarray object\n    consolidated = virtual_asset.extra_fields[\"zarr:consolidated\"]\n    zarr_format = virtual_asset.extra_fields[\"zarr:zarr_format\"]\n\n    return xr.open_zarr(session.store, consolidated=consolidated, zarr_format=zarr_format)\n\nNow let’s read the collection in from where we stored it in the json, find the latest version of the icechunk store, and lazily open it as an xarray.Dataset\n\ncollection = pystac.Collection.from_file(\"collection.json\")\n\nassets = collection.get_assets(media_type=\"application/vnd.zarr+icechunk\", role=\"latest-version\")\nassets\n\n{'nldas-3@YTNGFY4WY9189GEH1FNG': &lt;Asset href=s3://nasa-waterinsight/virtual-zarr-store/NLDAS-3-icechunk/&gt;}\n\n\n\nds = read_virtual_asset(collection, 'nldas-3@YTNGFY4WY9189GEH1FNG')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 51TB\nDimensions:   (time: 8399, lat: 6500, lon: 11700)\nCoordinates:\n  * lat       (lat) float64 52kB 7.005 7.015 7.025 7.035 ... 71.97 71.98 71.99\n  * lon       (lon) float64 94kB -169.0 -169.0 -169.0 ... -52.03 -52.01 -52.0\n  * time      (time) datetime64[ns] 67kB 2001-01-02 2001-01-03 ... 2024-01-01\nData variables:\n    Tair_max  (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    PSurf     (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    SWdown    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Wind_E    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Qair      (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Rainf     (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Wind_N    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    LWdown    (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Tair      (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\n    Tair_min  (time, lat, lon) float64 5TB dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;\nAttributes: (12/17)\n    missing_value:          -9999.0\n    time_definition:        daily\n    shortname:              NLDAS_FOR0010_D_3.0\n    title:                  NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3...\n    version:                3.0 beta\n    institution:            NASA GSFC\n    ...                     ...\n    websites:               https://ldas.gsfc.nasa.gov/nldas/v3/ ; https://li...\n    MAP_PROJECTION:         EQUIDISTANT CYLINDRICAL\n    SOUTH_WEST_CORNER_LAT:  7.005000114440918\n    SOUTH_WEST_CORNER_LON:  -168.9949951171875\n    DX:                     0.009999999776482582\n    DY:                     0.009999999776482582xarray.DatasetDimensions:time: 8399lat: 6500lon: 11700Coordinates: (3)lat(lat)float647.005 7.015 7.025 ... 71.98 71.99units :degrees_northstandard_name :latitudelong_name :latitudevmin :7.005000114440918vmax :71.9949951171875array([ 7.005   ,  7.015   ,  7.025   , ..., 71.974998, 71.984993, 71.994995],\n      shape=(6500,))lon(lon)float64-169.0 -169.0 ... -52.01 -52.0units :degrees_eaststandard_name :longitudelong_name :longitudevmin :-168.9949951171875vmax :-52.00499725341797array([-168.994995, -168.985001, -168.974991, ...,  -52.025002,  -52.014999,\n        -52.004997], shape=(11700,))time(time)datetime64[ns]2001-01-02 ... 2024-01-01long_name :timetime_increment :one daybegin_date :20010101begin_time :000000end_date :20010101end_time :235959array(['2001-01-02T00:00:00.000000000', '2001-01-03T00:00:00.000000000',\n       '2001-01-04T00:00:00.000000000', ..., '2023-12-30T00:00:00.000000000',\n       '2023-12-31T00:00:00.000000000', '2024-01-01T00:00:00.000000000'],\n      shape=(8399,), dtype='datetime64[ns]')Data variables: (10)Tair_max(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Daily maximum near surface air temperaturelong_name :Daily maximum 2-meter above ground Temperaturecell_methods :time: maximumvmin :230.1659393310547vmax :312.7441711425781\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nPSurf(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Pastandard_name :Surface pressurelong_name :Surface pressurecell_methods :time: meanvmin :46024.41796875vmax :103161.1796875\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nSWdown(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :W m-2standard_name :Surface incident shortwave radiationlong_name :Shortwave radiation flux downwards (surface)cell_methods :time: meanvmin :0.0vmax :288.2958068847656\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nWind_E(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :m s-1standard_name :Near surface eastward wind componentlong_name :10-meter above ground Zonal wind speedcell_methods :time: meanvmin :-12.775843620300293vmax :11.65600872039795\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nQair(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :kg kg-1standard_name :Near surface specific humiditylong_name :2-meter above ground Specific humiditycell_methods :time: meanvmin :5.8038440329255536e-05vmax :0.021006619557738304\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nRainf(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :kg m-2standard_name :Total precipitationlong_name :Total precipitationcell_methods :time: sumvmin :0.0vmax :60.617774963378906\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nWind_N(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :m s-1standard_name :Near surface northward wind componentlong_name :10-meter above ground Meridional wind speedcell_methods :time: meanvmin :-10.617785453796387vmax :11.41434097290039\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nLWdown(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :W m-2standard_name :Surface incident longwave radiationlong_name :Longwave radiation flux downwards (surface)cell_methods :time: meanvmin :126.3478012084961vmax :450.3310546875\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nTair(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Near surface air temperaturelong_name :2-meter above ground Temperaturecell_methods :time: meanvmin :228.31163024902344vmax :304.30316162109375\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nTair_min(time, lat, lon)float64dask.array&lt;chunksize=(1, 500, 900), meta=np.ndarray&gt;units :Kstandard_name :Daily minimum near surface air temperaturelong_name :Daily minimum 2-meter above ground Temperaturecell_methods :time: minimumvmin :227.1617889404297vmax :300.4527282714844\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([7.005000114440918, 7.015000343322754, 7.025000095367432,\n       7.035000324249268, 7.045000076293945, 7.055000305175781,\n       7.065000057220459, 7.075000286102295, 7.085000038146973,\n       7.095000267028809,\n       ...\n       71.90499877929688, 71.91499328613281, 71.92499542236328,\n       71.93499755859375, 71.94499206542969, 71.95499420166016,\n       71.96499633789062,  71.9749984741211, 71.98499298095703,\n        71.9949951171875],\n      dtype='float64', name='lat', length=6500))lonPandasIndexPandasIndex(Index([ -168.9949951171875, -168.98500061035156, -168.97499084472656,\n       -168.96499633789062,  -168.9550018310547,  -168.9449920654297,\n       -168.93499755859375, -168.92498779296875,  -168.9149932861328,\n       -168.90499877929688,\n       ...\n       -52.095001220703125, -52.084999084472656,  -52.07499694824219,\n        -52.06499481201172,  -52.05500030517578,  -52.04499816894531,\n       -52.034996032714844, -52.025001525878906,  -52.01499938964844,\n        -52.00499725341797],\n      dtype='float64', name='lon', length=11700))timePandasIndexPandasIndex(DatetimeIndex(['2001-01-02', '2001-01-03', '2001-01-04', '2001-01-05',\n               '2001-01-06', '2001-01-07', '2001-01-08', '2001-01-09',\n               '2001-01-10', '2001-01-11',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', name='time', length=8399, freq=None))Attributes: (17)missing_value :-9999.0time_definition :dailyshortname :NLDAS_FOR0010_D_3.0title :NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3.0 - *BETA*version :3.0 betainstitution :NASA GSFCdoi :TBDhistory :created on date: 2025-06-07T15:04:40reference :Maina-et-al_to-be-submitted; doi:TBDconventions :CF-1.6standard :ALMA v3 ; https://www.lmd.jussieu.fr/~polcher/ALMA/websites :https://ldas.gsfc.nasa.gov/nldas/v3/ ; https://lis.gsfc.nasa.gov/MAP_PROJECTION :EQUIDISTANT CYLINDRICALSOUTH_WEST_CORNER_LAT :7.005000114440918SOUTH_WEST_CORNER_LON :-168.9949951171875DX :0.009999999776482582DY :0.009999999776482582\n\n\nMake sure you can actually access underlying data:\n\n%%time\n\ncape_rain = ds.Rainf.sel(\n    time=\"2023-07-16\",\n    lat=slice(41.48, 42.10),\n    lon=slice(-70.84, -69.77)\n).compute()\n\nCPU times: user 1.01 s, sys: 298 ms, total: 1.31 s\nWall time: 4.75 s\n\n\n\ncape_rain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'Rainf' (lat: 62, lon: 107)&gt; Size: 53kB\narray([[ 5.16139793,  4.73033953,  4.79954004, ...,  2.80910897,\n         2.8472836 ,  2.64635181],\n       [ 4.856112  ,  4.68789768,  4.71580458, ...,  2.80869484,\n         2.87434697,  2.80177665],\n       [ 4.72029495,  4.83154964,  4.46945238, ...,  2.90991831,\n         3.02153993,  3.10612035],\n       ...,\n       [ 7.01298904,  7.19748259,  7.40456963, ..., 18.69691277,\n        18.66467857, 18.25347519],\n       [ 7.89184809,  8.40285873,  7.87267876, ..., 17.93091774,\n        19.15982437, 18.79140854],\n       [ 8.36418152,  8.91619682,  8.63145733, ..., 18.75998878,\n        20.46075821, 20.56634521]], shape=(62, 107))\nCoordinates:\n  * lat      (lat) float64 496B 41.49 41.49 41.51 41.51 ... 42.08 42.08 42.1\n  * lon      (lon) float64 856B -70.83 -70.82 -70.81 ... -69.79 -69.78 -69.77\n    time     datetime64[ns] 8B 2023-07-16\nAttributes:\n    units:          kg m-2\n    standard_name:  Total precipitation\n    long_name:      Total precipitation\n    cell_methods:   time: sum\n    vmin:           0.0\n    vmax:           60.617774963378906xarray.DataArray'Rainf'lat: 62lon: 1075.161 4.73 4.8 4.876 4.935 5.637 ... 17.67 17.78 18.76 20.46 20.57array([[ 5.16139793,  4.73033953,  4.79954004, ...,  2.80910897,\n         2.8472836 ,  2.64635181],\n       [ 4.856112  ,  4.68789768,  4.71580458, ...,  2.80869484,\n         2.87434697,  2.80177665],\n       [ 4.72029495,  4.83154964,  4.46945238, ...,  2.90991831,\n         3.02153993,  3.10612035],\n       ...,\n       [ 7.01298904,  7.19748259,  7.40456963, ..., 18.69691277,\n        18.66467857, 18.25347519],\n       [ 7.89184809,  8.40285873,  7.87267876, ..., 17.93091774,\n        19.15982437, 18.79140854],\n       [ 8.36418152,  8.91619682,  8.63145733, ..., 18.75998878,\n        20.46075821, 20.56634521]], shape=(62, 107))Coordinates: (3)lat(lat)float6441.49 41.49 41.51 ... 42.08 42.1units :degrees_northstandard_name :latitudelong_name :latitudevmin :7.005000114440918vmax :71.9949951171875array([41.485001, 41.494999, 41.505001, 41.514999, 41.525002, 41.535   ,\n       41.545002, 41.555   , 41.564999, 41.575001, 41.584999, 41.595001,\n       41.605   , 41.615002, 41.625   , 41.635002, 41.645   , 41.654999,\n       41.665001, 41.674999, 41.685001, 41.695   , 41.705002, 41.715   ,\n       41.724998, 41.735001, 41.744999, 41.755001, 41.764999, 41.775002,\n       41.785   , 41.795002, 41.805   , 41.814999, 41.825001, 41.834999,\n       41.845001, 41.855   , 41.865002, 41.875   , 41.885002, 41.895   ,\n       41.904999, 41.915001, 41.924999, 41.935001, 41.945   , 41.955002,\n       41.965   , 41.974998, 41.985001, 41.994999, 42.005001, 42.014999,\n       42.025002, 42.035   , 42.045002, 42.055   , 42.064999, 42.075001,\n       42.084999, 42.095001])lon(lon)float64-70.83 -70.82 ... -69.78 -69.77units :degrees_eaststandard_name :longitudelong_name :longitudevmin :-168.9949951171875vmax :-52.00499725341797array([-70.834999, -70.824997, -70.814995, -70.805   , -70.794998, -70.784996,\n       -70.774994, -70.764999, -70.754997, -70.744995, -70.735001, -70.724998,\n       -70.714996, -70.704994, -70.695   , -70.684998, -70.674995, -70.665001,\n       -70.654999, -70.644997, -70.634995, -70.625   , -70.614998, -70.604996,\n       -70.594994, -70.584999, -70.574997, -70.564995, -70.555   , -70.544998,\n       -70.534996, -70.524994, -70.514999, -70.504997, -70.494995, -70.485001,\n       -70.474998, -70.464996, -70.454994, -70.445   , -70.434998, -70.424995,\n       -70.415001, -70.404999, -70.394997, -70.384995, -70.375   , -70.364998,\n       -70.354996, -70.344994, -70.334999, -70.324997, -70.314995, -70.305   ,\n       -70.294998, -70.284996, -70.274994, -70.264999, -70.254997, -70.244995,\n       -70.235001, -70.224998, -70.214996, -70.204994, -70.195   , -70.184998,\n       -70.174995, -70.165001, -70.154999, -70.144997, -70.134995, -70.125   ,\n       -70.114998, -70.104996, -70.094994, -70.084999, -70.074997, -70.064995,\n       -70.055   , -70.044998, -70.034996, -70.024994, -70.014999, -70.004997,\n       -69.994995, -69.985001, -69.974998, -69.964996, -69.954994, -69.945   ,\n       -69.934998, -69.924995, -69.915001, -69.904999, -69.894997, -69.884995,\n       -69.875   , -69.864998, -69.854996, -69.844994, -69.834999, -69.824997,\n       -69.814995, -69.805   , -69.794998, -69.784996, -69.774994])time()datetime64[ns]2023-07-16long_name :timetime_increment :one daybegin_date :20010101begin_time :000000end_date :20010101end_time :235959array('2023-07-16T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latPandasIndexPandasIndex(Index([ 41.48500061035156, 41.494998931884766, 41.505001068115234,\n        41.51499938964844, 41.525001525878906,  41.53499984741211,\n        41.54500198364258,  41.55500030517578, 41.564998626708984,\n        41.57500076293945, 41.584999084472656, 41.595001220703125,\n        41.60499954223633,   41.6150016784668,             41.625,\n        41.63500213623047,  41.64500045776367, 41.654998779296875,\n       41.665000915527344,  41.67499923706055, 41.685001373291016,\n        41.69499969482422,  41.70500183105469,  41.71500015258789,\n       41.724998474121094,  41.73500061035156, 41.744998931884766,\n       41.755001068115234,  41.76499938964844, 41.775001525878906,\n        41.78499984741211,  41.79500198364258,  41.80500030517578,\n       41.814998626708984,  41.82500076293945, 41.834999084472656,\n       41.845001220703125,  41.85499954223633,   41.8650016784668,\n                   41.875,  41.88500213623047,  41.89500045776367,\n       41.904998779296875, 41.915000915527344,  41.92499923706055,\n       41.935001373291016,  41.94499969482422,  41.95500183105469,\n        41.96500015258789, 41.974998474121094,  41.98500061035156,\n       41.994998931884766, 42.005001068115234,  42.01499938964844,\n       42.025001525878906,  42.03499984741211,  42.04500198364258,\n        42.05500030517578, 42.064998626708984,  42.07500076293945,\n       42.084999084472656, 42.095001220703125],\n      dtype='float64', name='lat'))lonPandasIndexPandasIndex(Index([-70.83499908447266, -70.82499694824219, -70.81499481201172,\n       -70.80500030517578, -70.79499816894531, -70.78499603271484,\n       -70.77499389648438, -70.76499938964844, -70.75499725341797,\n        -70.7449951171875,\n       ...\n       -69.86499786376953, -69.85499572753906,  -69.8449935913086,\n       -69.83499908447266, -69.82499694824219, -69.81499481201172,\n       -69.80500030517578, -69.79499816894531, -69.78499603271484,\n       -69.77499389648438],\n      dtype='float64', name='lon', length=107))Attributes: (6)units :kg m-2standard_name :Total precipitationlong_name :Total precipitationcell_methods :time: sumvmin :0.0vmax :60.617774963378906",
    "crumbs": [
      "Examples",
      "STAC collection for virtual store"
    ]
  },
  {
    "objectID": "examples/04_STAC_Items_for_legacy_files_in_Virtual_Icechunk_Store.html",
    "href": "examples/04_STAC_Items_for_legacy_files_in_Virtual_Icechunk_Store.html",
    "title": "Creating STAC Items for a legacgy files referenced by Virtual Icechunk Store",
    "section": "",
    "text": "There is a virtual icechunk store that is publicly available at: s3://nasa-waterinsight/virtual-zarr-store/NLDAS-3-icechunk/\nThis notebook goes through the current thinking for how you would set up STAC items that point to the legacy file formats referenced by the virtual store.\nimport icechunk\nimport zarr\nimport xstac\n\nimport xarray as xr\nZarr can emit a lot of warnings about Numcodecs not being including in the Zarr version 3 specification yet – let’s suppress those.\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n    category=UserWarning,\n)\nThese are the PRs that need to land before you can open the virtual icechunk store with zarr directly:\nUntil then:\nstorage = icechunk.s3_storage(\n    bucket=\"nasa-waterinsight\",\n    prefix=\"virtual-zarr-store/NLDAS-3-icechunk/\",\n    region=\"us-west-2\",\n    anonymous=True,\n)\n\nconfig = icechunk.RepositoryConfig.default()\nconfig.set_virtual_chunk_container(\n    icechunk.VirtualChunkContainer(\n        \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\",\n        icechunk.s3_store(region=\"us-west-2\")\n    )\n)\nvirtual_credentials = icechunk.containers_credentials(\n    {\n        \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\": icechunk.s3_anonymous_credentials()\n    }\n)\n\nrepo = icechunk.Repository.open(\n    storage=storage,\n    config=config,\n    authorize_virtual_chunk_access=virtual_credentials,\n)\n\nsession = repo.readonly_session('main')\nds = xr.open_zarr(session.store, consolidated=False, zarr_format=3)\nFrom this virtual icechunk store we can get the locations of all the chunks. This technically only accesses a single snapshot, so to get them all you might have to recurse back through time (https://github.com/earth-mover/icechunk/issues/1194). In this case though\nNote that this takes some time.\n%%time\n\nchunk_locations = session.all_virtual_chunk_locations()\n\nCPU times: user 24.2 s, sys: 3.16 s, total: 27.4 s\nWall time: 47.1 s\nlen(chunk_locations)\n\n14194310\nThat gives the locations of every chunk. So we need to deduplicate it to get a list of files\nlegacy_files = sorted(list(set(chunk_locations)))\nlegacy_files[:5]\n\n['s3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc',\n 's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010102.030.beta.nc',\n 's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010103.030.beta.nc',\n 's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010104.030.beta.nc',\n 's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010105.030.beta.nc']\nAs a sanity check we can just confirm that we have the same number of files as we do timesteps.\nassert len(legacy_files) == len(ds.time)\nFor instance here is the chunkgrid for ds.PSurf\nds.PSurf.data.numblocks\n\n(8399, 13, 13)\nYou can multiply all those numbers together to get the total number of chunks\nnumchunks = 1 \nfor n in ds.PSurf.data.numblocks:\n    numchunks *= n\nnumchunks\n\n1419431\nThat number matches the number in the dask array representation.\nds.PSurf.data\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.65 TiB\n3.43 MiB\n\n\nShape\n(8399, 6500, 11700)\n(1, 500, 900)\n\n\nDask graph\n1419431 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             11700 6500 8399\nBut we can’t really know what the flat list of chunk_locations means in the context of that chunk grid. What does this mean?\nchunk_locations[0]\n\n's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc'\nOk so you can’t really get that information out of icechunk right now as far as I can tell. So what about the other piece.\nFirst let’s take a slice of the array that represents just one point in time. As long as all the variables have the same shape it shouldn’t matter which one you pick.\na = ds.PSurf.data[0, :, :]\na\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n580.22 MiB\n3.43 MiB\n\n\nShape\n(6500, 11700)\n(500, 900)\n\n\nDask graph\n169 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                 11700 6500\nlat_chunk_indices = np.insert(np.array(a.chunks[0]).cumsum() - 1, 0, 0)\nlat_chunk_bounds = ds.lat.data[lat_chunk_indices]\nlat_chunk_bounds\n\narray([ 7.00500011, 11.99499989, 16.99499893, 21.99499893, 26.99499893,\n       31.99499893, 36.99499893, 41.99499893, 46.99499893, 51.99499893,\n       56.99499893, 61.99499893, 66.99499512, 71.99499512])\nlon_chunk_indices = np.insert(np.array(a.chunks[1]).cumsum() - 1, 0, 0)\nlon_chunk_bounds = ds.lon.data[lon_chunk_indices]\nlon_chunk_bounds\n\narray([-168.99499512, -160.00498962, -151.00498962, -142.00498962,\n       -133.00500488, -124.00499725, -115.00499725, -106.00499725,\n        -97.00499725,  -88.00499725,  -79.00499725,  -70.00499725,\n        -61.00499725,  -52.00499725])\nSo those are the min and max of each chunk. We can construct the bounding box for each chunk by stepping along the lats and lons and taking two items at a time. We’ll start with an array of zeros that has the same shape as the chunk grid except the last dimension is size 4 so that it can contain the bboxes.\nds.PSurf.dims\n\n('time', 'lat', 'lon')\nbboxes = np.zeros((a.numblocks[0], a.numblocks[1], 4))\n\nfor n in range(a.numblocks[0]):\n    for m in range(a.numblocks[1]):\n        bboxes[n, m] = [lon_chunk_bounds[m], lat_chunk_bounds[n], lon_chunk_bounds[m + 1], lat_chunk_bounds[n + 1]]\nbboxes[0, 0]\n\narray([-168.99499512,    7.00500011, -160.00498962,   11.99499989])\nIn practice we probably don’t need the bounding boxes for each chunk. We just need the bounding boxes for the legacy files which we can assume contain contiguous chunks.\nds.rio.bounds()\n\n(-168.9999951170962, 7.000000114825381, -51.99999725350927, 71.99999511680303)",
    "crumbs": [
      "Examples",
      "STAC items for legacy files referenced by virtual store"
    ]
  },
  {
    "objectID": "examples/04_STAC_Items_for_legacy_files_in_Virtual_Icechunk_Store.html#create-stac-items",
    "href": "examples/04_STAC_Items_for_legacy_files_in_Virtual_Icechunk_Store.html#create-stac-items",
    "title": "Creating STAC Items for a legacgy files referenced by Virtual Icechunk Store",
    "section": "Create STAC items",
    "text": "Create STAC items\nIn order to construct a STAC Item for each of these legacy files we need to have the have the temporal and spatial bounds for each file. Unlike the STAC Collection we want the metadata in these to be pretty minimal.\nJust as a quick note we are using pystac here for construction and using rustac for reading/writing to stac-geoparquet.\n\nimport json\nimport datetime as dt\n\nimport pystac\nimport rustac\n\nFirst we will read in the collection that we had created in the previous notebook:\n\ncollection = pystac.Collection.from_file(\"./collection.json\")\n\nWe’ll use some hard-coded values that are pulled straight from the xr.Dataset. For instance we know that the bounding box for each of these netcdf file is exactly the same they just each represent a different timestep.\n\nfrom shapely.geometry import box\nfrom shapely import to_geojson\n\nbbox = ds.rio.bounds()\nbbox = tuple(round(b, 4) for b in bbox)  # it might be wrong to round this, but it looked sooo close to round.\nbbox_polygon = box(*bbox)\n\nbegin_time = ds.time.begin_time\nend_time = ds.time.end_time\n\nThe item-specific values we will deduce from the filename for now, but ideally we would get this information in a more systematic way from the chunk information. Let’s take a look at those legacy filepaths again:\n\nlegacy_files[0]\n\n's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc'\n\n\nSo - and this is kind of hacky - we can see that the whole date is actually encoded in the filepath. We just need to grab it out of there and parse it. Let’s create a function that creates an item that basically just has the spatial and temporal extents set and points to one particular legacy file on s3.\n\ndef create_item(legacy_filepath):\n    filename = legacy_filepath.split(\"/\")[-1]\n    date = filename.split(\".030.beta.\")[0][-8:]\n    \n    datetime = dt.datetime.strptime(date, \"%Y%m%d\")\n    start_datetime = dt.datetime.strptime(f\"{date} {begin_time}\", \"%Y%m%d %H%M%S\")\n    end_datetime = dt.datetime.strptime(f\"{date} {end_time}\", \"%Y%m%d %H%M%S\")\n    \n    return pystac.Item(\n        filename,\n        geometry=json.loads(to_geojson(bbox_polygon)),\n        properties={\n            \"storage:schemes\": collection.extra_fields[\"storage:schemes\"]\n        },\n        bbox=bbox,\n        datetime=datetime,\n        start_datetime=start_datetime,\n        end_datetime=end_datetime,\n        collection=collection,\n        stac_extensions=[\n            \"https://stac-extensions.github.io/storage/v2.0.0/schema.json\",\n        ],\n        assets={\n            \"data\": pystac.Asset(\n                href=legacy_filepath,\n                media_type=\"application/x-netcdf\",\n                roles=[\"data\"],\n                extra_fields={\n                    \"storage:refs\": [\n                        \"aws-s3-nasa-waterinsight\"\n                    ],\n                }\n            )\n        }\n    )\n\nWe’ll run it on one file to make sure it works and see how long it takes.\n\n%%time\n\nitem = create_item(legacy_files[0])\nitem\n\nCPU times: user 722 μs, sys: 0 ns, total: 722 μs\nWall time: 726 μs\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Feature\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.1.0\"\n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/storage/v2.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"NLDAS_FOR0010_D.A20010101.030.beta.nc\"\n        \n    \n                \n            \n                \n                    \n        \n            geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -52.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            7.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -52.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            72.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -169.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            72.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -169.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            7.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -52.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            7.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -169.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            7.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -52.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            72.0\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            properties\n            \n        \n            \n                \n        \n            storage:schemes\n            \n        \n            \n                \n        \n            aws-s3-nasa-waterinsight\n            \n        \n            \n                \n        \n            type\n            \"aws-s3\"\n        \n    \n            \n        \n            \n                \n        \n            platform\n            \"https://{bucket}.s3.{region}.amazonaws.com\"\n        \n    \n            \n        \n            \n                \n        \n            bucket\n            \"nasa-waterinsight\"\n        \n    \n            \n        \n            \n                \n        \n            region\n            \"us-west-2\"\n        \n    \n            \n        \n            \n                \n        \n            anonymous\n            True\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            start_datetime\n            \"2001-01-01T00:00:00Z\"\n        \n    \n            \n        \n            \n                \n        \n            end_datetime\n            \"2001-01-01T23:59:59Z\"\n        \n    \n            \n        \n            \n                \n        \n            datetime\n            \"2001-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        links[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"collection\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"/home/jsignell/NASA/dse-virtual-zarr-workshop/docs/examples/collection.json\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3.0 - *BETA*\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            href\n            \"s3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-netcdf\"\n        \n    \n            \n        \n            \n                \n        storage:refs[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"aws-s3-nasa-waterinsight\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            collection\n            \"nldas-3\"\n        \n    \n                \n            \n        \n    \n\n\n\nValidate the item:\n\nitem.validate()\n\n['https://schemas.stacspec.org/v1.1.0/item-spec/json-schema/item.json',\n 'https://stac-extensions.github.io/storage/v2.0.0/schema.json']\n\n\nNow instead of just making one of these we’ll want to make a whole bunch (8399 to be exact) so instead of storing each as its own json blob I’ll save them as an item collection in stac-geoparquet.\n\n%%time\n\nitems = [\n    create_item(legacy_file).to_dict(include_self_link=False, transform_hrefs=False)\n    for legacy_file in legacy_files\n]\n\nCPU times: user 579 ms, sys: 5.4 ms, total: 584 ms\nWall time: 584 ms\n\n\n\nawait rustac.write(\"items.parquet\", items)\n\n{'e_tag': '2867791-63deba937e261-53b1f', 'version': None}\n\n\nRead it back in just to prove that we can:\n\nitem_collection = await rustac.read(\"items.parquet\")\nitem_collection[\"features\"][0]\n\n{'type': 'Feature',\n 'stac_version': '1.1.0',\n 'stac_extensions': ['https://stac-extensions.github.io/storage/v2.0.0/schema.json'],\n 'id': 'NLDAS_FOR0010_D.A20010101.030.beta.nc',\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-52.0, 7.0],\n    [-52.0, 72.0],\n    [-169.0, 72.0],\n    [-169.0, 7.0],\n    [-52.0, 7.0]]]},\n 'bbox': (-169.0, 7.0, -52.0, 72.0),\n 'properties': {'datetime': '2001-01-01T00:00:00Z',\n  'start_datetime': '2001-01-01T00:00:00Z',\n  'end_datetime': '2001-01-01T23:59:59Z',\n  'storage:schemes': {'aws-s3-nasa-waterinsight': {'type': 'aws-s3',\n    'platform': 'https://{bucket}.s3.{region}.amazonaws.com',\n    'bucket': 'nasa-waterinsight',\n    'region': 'us-west-2',\n    'anonymous': True}}},\n 'links': [{'href': '/home/jsignell/NASA/dse-virtual-zarr-workshop/docs/examples/collection.json',\n   'rel': 'collection',\n   'type': 'application/json',\n   'title': 'NLDAS Forcing Data L4 Daily 0.01 x 0.01 degree V3.0 - *BETA*'}],\n 'assets': {'data': {'href': 's3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc',\n   'type': 'application/x-netcdf',\n   'roles': ['data'],\n   'storage:refs': ['aws-s3-nasa-waterinsight']}},\n 'collection': 'nldas-3'}",
    "crumbs": [
      "Examples",
      "STAC items for legacy files referenced by virtual store"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VirtualiZarr examples",
    "section": "",
    "text": "This workshop will include a mostly self-guided exploration of VirtualiZarr 2.0 for either pre-constructed examples or your own data.\nYou can find out more about VirtualiZarr on readthedocs and explore the source code on GitHub. If you’re familiar with VirtualiZarr 1.0, check out the migration guide for information about how this new release differs.\nYou can download the source code from GitHub:\ngit clone git@github.com:virtual-zarr/esip-2025.git\ncd esip-2025\nYou can use pixi to create an environment for these examples. After installing pixi, create an environment from the lock file via:\npixi install",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "VirtualiZarr examples",
    "section": "",
    "text": "This workshop will include a mostly self-guided exploration of VirtualiZarr 2.0 for either pre-constructed examples or your own data.\nYou can find out more about VirtualiZarr on readthedocs and explore the source code on GitHub. If you’re familiar with VirtualiZarr 1.0, check out the migration guide for information about how this new release differs.\nYou can download the source code from GitHub:\ngit clone git@github.com:virtual-zarr/esip-2025.git\ncd esip-2025\nYou can use pixi to create an environment for these examples. After installing pixi, create an environment from the lock file via:\npixi install",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "VirtualiZarr examples",
    "section": "License",
    "text": "License\nContent in this repository is licensed under the MIT License.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "examples/01_AWS_Public_Data_Program_NetCDF.html",
    "href": "examples/01_AWS_Public_Data_Program_NetCDF.html",
    "title": "Walk-through - Virtualizing NetCDFs from Amazon’s Open Data Program",
    "section": "",
    "text": "Let’s start with the first example from the VirtualiZarr homepage.\nThis example uses the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) from the Registry of Open Data on AWS. The virtualization process will be much faster if run proximal to the data in AWS’s us-west-2 region.\nCreating the virtual dataset looks quite similar to how we normally open data with xarray, but there are a few notable differences that are shown through this example.\nFirst, import the necessary functions and classes:\n\nimport icechunk\nimport obstore\nfrom virtualizarr import open_virtual_dataset, open_virtual_mfdataset\nfrom virtualizarr.parsers import HDFParser\nfrom virtualizarr.registry import ObjectStoreRegistry\n\nZarr can emit a lot of warnings about Numcodecs not being including in the Zarr version 3 specification yet – let’s suppress those.\n\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n    category=UserWarning,\n)\n\nWe can use Obstore’s obstore.store.from_url convenience method to create an ObjectStore that can fetch data from the specified URLs.\n\nbucket = \"s3://nex-gddp-cmip6\"\npath = \"NEX-GDDP-CMIP6/ACCESS-CM2/ssp126/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_ssp126_r1i1p1f1_gn_2015_v2.0.nc\"\nstore = obstore.store.from_url(bucket, region=\"us-west-2\", skip_signature=True)\n\nWe also need to create an ObjectStoreRegistry that maps the URL structure to the ObjectStore.\n\nregistry = ObjectStoreRegistry({bucket: store})\n\nNow, let’s create a parser instance and create a virtual dataset by passing the URL, parser, and registry to virtualizarr.open_virtual_dataset.\n\nparser = HDFParser()\nvds = open_virtual_dataset(\n    url=f\"{bucket}/{path}\",\n    parser=parser,\n    registry=registry,\n    loadable_variables=[],\n)\nprint(vds)\n\nSince we specified loadable_variables=[], no data has been loaded or copied in this process. We have merely created an in-memory lookup table that points to the location of chunks in the original netCDF when data is needed later on. The default behavior (loadable_variables=None) will load data associated with coordinates but not data variables. The size represents the size of the original dataset - you can see the size of the virtual dataset using the vz accessor:\n\nprint(f\"Original dataset size: {vds.nbytes} bytes\")\nprint(f\"Virtual dataset size: {vds.vz.nbytes} bytes\")\n\nVirtualiZarr’s other top-level function is virtualizarr.open_virtual_mfdataset, which can open and virtualize multiple data sources into a single virtual dataset, similar to how xarray.open_mfdataset opens multiple data files as a single dataset.\n\nurls = [\n    f\"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/ACCESS-CM2/ssp126/r1i1p1f1/tasmax/tasmax_day_ACCESS-CM2_ssp126_r1i1p1f1_gn_{year}_v2.0.nc\"\n    for year in range(2015, 2017)\n]\nvds = open_virtual_mfdataset(urls, parser=parser, registry=registry)\nprint(vds)\n\nThe magic of VirtualiZarr is that you can persist the virtual dataset to disk in a chunk references format such as Icechunk, meaning that the work of constructing the single coherent dataset only needs to happen once. For subsequent data access, you can use xarray.open_zarr to open that Icechunk store, which on object storage is far faster than using xarray.open_mfdataset to open the the original non-cloud-optimized files.\nLet’s persist the Virtual dataset using Icechunk. Here we store the dataset in a memory store but in most cases you’ll store the virtual dataset in the cloud.\n\nicechunk_store = icechunk.in_memory_storage()\nrepo = icechunk.Repository.create(icechunk_store)\nsession = repo.writable_session(\"main\")\nvds.vz.to_icechunk(session.store)\nsession.commit(\"Create virtual store\")",
    "crumbs": [
      "Examples",
      "Walk through - NetCDFs on AWS"
    ]
  },
  {
    "objectID": "examples/02_ESGF_NetCDF_Solution.html",
    "href": "examples/02_ESGF_NetCDF_Solution.html",
    "title": "Hands-on - Virtualize NetCDF from ESGF",
    "section": "",
    "text": "This example uses data from the Earth System Grid Federation THREDDS Data Server.\nThis is a quicker example for hands-on experience. For a full walkthrough, view previous example on the NASA-NEX-GDDP CMIP6 data.\nThank you to Raphael Hagen for contributing this example!\n\nStep 1: Import necessary functions and classes\n\nfrom obstore.store import HTTPStore\nfrom virtualizarr import open_virtual_dataset\nfrom virtualizarr.parsers import HDFParser\nfrom virtualizarr.registry import ObjectStoreRegistry\n\n\n\nStep 2: Define data location\n\nbucket = \"https://esgf-data.ucar.edu\"\npath = \"thredds/fileServer/esg_dataroot/CMIP6/CMIP/NCAR/CESM2/historical/r3i1p1f1/day/tas/gn/v20190308/tas_day_CESM2_historical_r3i1p1f1_gn_19200101-19291231.nc\"\n\n\n\nStep 3: Create an ObjectStore and an ObjectStoreRegistry\n\nstore = HTTPStore.from_url(bucket)\nregistry = ObjectStoreRegistry({bucket: store})\n\n\n\nStep 4: Create an instance of the HDFParser\n\nparser = HDFParser()\n\n\n\nStep 5: Create a virtual dataset via open_virtual_dataset\n\nvds = open_virtual_dataset(\n    url=f\"{bucket}/{path}\",\n    parser=parser,\n    registry=registry,\n    loadable_variables=[\"lat\", \"lon\", \"time\", \"time_bnds\", \"lat_bnds\", \"lon_bnds\"],\n)\n\n\nvds",
    "crumbs": [
      "Examples",
      "Hands on - NetCDFs from ESGF"
    ]
  }
]